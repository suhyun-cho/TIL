{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN 알고리즘 \n",
    "2020-03-24\n",
    "\n",
    "### 모델 개요\n",
    "> KNN은 새로운 데이터가 주어졌을 때 기존 데이터 가운데 가장 가까운 k개 이웃의 정보로 새로운 데이터를 예측하는 방법론입니다.\n",
    "\n",
    "<img src=\"./pic/KNN_algorithm.PNG\" width=\"300\">\n",
    "\n",
    "#### KNN의 하이퍼파라메터(Hyper parameter)\n",
    "> * 탐색할 이웃 수(k)\n",
    "    * k가 작을 경우 데이터의 지역적 특성을 지나치게 반영하게 됩니다(overfitting). 반대로 매우 클 경우 모델이 과하게 정규화되는 경향이 있습니다(underfitting). k의 크기에 따라 분류 경계면이 단순해진다.\n",
    "> * 거리 측정 방법.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 거리 지표\n",
    "> * Euclidean Distance (유클리디언 거리)\n",
    "    * 가장 흔히 사용하는 거리 척도입니다. 두 관측치 사이의 직선 최단거리를 의미 <br>\n",
    "    \n",
    "<img src=\"./pic/유클리디언거리.PNG\" width=\"300\">\n",
    "<br>\n",
    "\n",
    "> * Manhattan Distance (맨하탄 거리)\n",
    "    * A에서 B로 이동할 때 각 좌표축 방향으로만 이동할 경우에 계산되는 거리입니다. \n",
    "> * Mahalanobis Distance (마할라노비스 거리)\n",
    "    * 변수 내 분산, 변수 간 공분산을 모두 반영하여 거리를 계산하는 방식입니다. 변수 간 상관관계를 고려한 거리 지표입니다.\n",
    "> * Correlation Distance\n",
    "    * 데이터의 pearson correlation을 거리 척도로 직접 사용합니다. 개별 관측치 하나하나가 아니라 데이터 전체의 경향성을 비교하기 위한 척도입니다. \n",
    "> * Rank Correlation Distance\n",
    "\n",
    "### combining rule\n",
    "1-NN을 제외한 KNN은 주변 이웃의 분포에 따라 예측 결과가 충분히 달라질 수 있다.<br>\n",
    "> * **다수결**\n",
    "    * 가장 단순한 결정 방식은 다수결(Majority voting)이다. 이웃들 범주 가운데 빈도 기준 제일 많은 범주로 새 데이터의 범주를 예측하는 것.\n",
    "\n",
    "> * **가중합(weighted voting) 방식**\n",
    "    * 거리(d)가 가까운(=유사도가 높은) 이웃의 정보에 좀 더 가중치를 줍니다. 1/(1+d), 1/(1+d2), exp(−d) 등 단조감소함수이기만 하면 무엇이든 가중치 산출 함수로 쓸 수 있다고 합니다.\n",
    "    \n",
    "### cut-off 기준 설정\n",
    "범주 간 비율이 불균형한 데이터일 땐 여기에 주의를 해야 합니다. <br>\n",
    "예로, 제조업 정상/불량 데이터를 분류하는 문제의 경우 학습데이터에선 정상 관측치가 대다수일 겁니다.<br>\n",
    "여기에서 새 관측치 주변 이웃의 범주 비율 정보가 ‘정상 : 0.8, 불량 : 0.2’이라면 불량으로 판정하는 게 합리적일 것\n",
    "\n",
    "* 컷오프 기준을 설정할 때 학습데이터 범주의 사전확률(prior probability)을 고려해야 한다는 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 수행시 주의점\n",
    "* KNN 수행 전 반드시 변수를 **정규화(Normalization)**해 주어야 합니다. \n",
    "    * 예시 : 도시별 정보를 모아서 유사한 환경을 지닌 도시를 뽑는 문제를 풀어봅시다.거리/유사성 측정시 미세먼지농도 정보는 전혀 반영이 되지 않을 겁니다. 인구 변수에 해당하는 숫자가 훨씬 크기 때문입니다. <br>\n",
    "    따라서 변수별로 평균과 분산을 일치시키는 정규화 작업을 반드시 KNN 적용 전 수행해 주어야 합니다.\n",
    "    \n",
    "### KNN의 장단점\n",
    "**[장점]** <br>\n",
    "\n",
    "> KNN은 학습데이터 내에 끼어있는 노이즈의 영향을 크게 받지 않으며<br>\n",
    "학습데이터 수가 많다면 꽤 효과적인 알고리즘이라고 합니다. <br>\n",
    "* 특히 **마할라노비스 거리**와 같이 데이터의 분산을 고려할 경우 **매우 강건(robust)한 방법론**으로 알려져 있습니다.<br>\n",
    "\n",
    "\n",
    "**[단점]** <br>\n",
    "\n",
    "> 그러나 최적 이웃의 수(k)와 어떤 거리척도가 분석에 적합한지 불분명해 <br>\n",
    "데이터 각각의 특성에 맞게 연구자가 임의로 선정해야 하는 단점이 있습니다.\n",
    "\n",
    "> 또 새로운 관측치와 각각의 학습 데이터 사이의 거리를 전부 측정해야 하므로 계산 시간이 오래 걸리는 한계점이 존재.\n",
    "\n",
    "이 때문에 Locality Sensitive Hashing, Network based Indexer, Optimized product quantization 등 KNN의 계산복잡성을 줄이려는 시도들이 여럿 제안되었습니다. 인스턴스 간 거리를 모두 계산하지 않고도 마치 그렇게 한 것처럼 결과를 내는 방법론들입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suhyun3",
   "language": "python",
   "name": "suhyun3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
