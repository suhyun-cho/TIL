{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결정트리, 결정트리 앙상블(Bagging, Boosting)\n",
    "작성일자 : 2020-03-27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 결정트리**\n",
    "[참고] https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-4-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%ACDecision-Tree?category=1057680 <br>\n",
    "\n",
    "특정 기준(질문)에 따라 데이터를 구분하는 모델을 결정 트리 모델이라고 합니다.<br>\n",
    "한번의 분기 때마다 변수 영역을 두 개로 구분합니다.\n",
    "\n",
    "**[결정 트리 학습의 의미]**\n",
    "- 결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 질문의 목록을 학습한다는 뜻이다. <br>\n",
    "머신러닝에서 이런 질문들을 테스트라고 한다. <br>\n",
    "보통의 데이터는 예/아니오의 형태 특성으로 구성되지 않고, 2차원 데이터 세트와 같이 연속된 특성으로 구성된다. \n",
    "- 그렇기 때문에 연속적인 데이터에 적용할 테스트(질문)는 **'해당 특성이 특정한 값보다 큰가?'**와 같은 형태를 띤다.\n",
    "\n",
    "**[트리 구성 시 알고리즘]**\n",
    " - 트리를 만들 때의 알고리즘은 가능한 모든 테스트에서 **타겟 값에 대하여 가장 많은 정보를 가진 것**을 고른다.\n",
    "     - 즉, 거의 모든 테스트(질문)에서 데이터를 가장 잘 나누는 테스트를 찾는 것이다.\n",
    "--------------------------------------------------------------------------------------------------    \n",
    "### **가지치기(Pruning)**\n",
    "> 트리에 가지가 너무 많다면 오버피팅이라 볼 수 있습니다. <br>\n",
    "즉, 최대 깊이나 터미널 노드의 최대 개수, 혹은 한 노드가 분할하기 위한 최소 데이터 수를 제한하는 것입니다.\n",
    "* min_sample_split 파라미터 : 한 노드에 들어있는 최소 데이터 수를 정해줄 수 있습니다\n",
    "* max_depth를 통해서 최대 깊이를 지정해줄 수도 있습니다. \n",
    "\n",
    "###  **특성 중요도(Feature Importance)**\n",
    "> 트리를 만드는 결정에 각 특성이 얼마나 중요한지를 평가하는 것이다.<br>\n",
    "- 이 값은 0과 1사이의 숫자로, 각 특성에 대해 0은 전혀 사용되지 않았다는 뜻이고 1은 완벽하게 타겟 클래스를 예측했다는 뜻이다. \n",
    "- 특성 중요도 전체의 합은 1이다. \n",
    "    - 특성 중요도가 높으면 트리 구조에서 높은 층에 해당하는 노드의 테스트(질문)라는 것이고 \n",
    "    - 특성 중요도가 낮으면 낮은 층에 해당하는, 세세하게 클래스를 분류하는 노드의 테스트(질문)이거나 <br>\n",
    "    혹은 테스트에 사용되지 않는 특성이라는 것이다.\n",
    "- 그러나 특성 중요도가 낮다고 하여 그 특성이 유용하지 않다는 것은 아니다.\n",
    "    - 단지 트리가 그 특성을 선택하지 않았을 뿐이며, 다른 특성이 동일한 정보를 지니고있어서일 수 있다\n",
    "    \n",
    "### **매개변수**\n",
    "결정 트리에서 모델의 복잡도를 조절하는 매개변수는 트리가 완전히 만들어지기 전에 멈추도록 하는 사전 가지치기 매개변수이다. <br>\n",
    "아래 중에서 하나만 지정해주어도 과대적합(Overfitting)을 막는 데는 충분하다.\n",
    "\n",
    "- 최대 깊이(Max depth)\n",
    "- 최대 리프 노드의 개수(Max leaf nodes)\n",
    "- 최소 한도(Min samples leaf) :노드가 분할하기 위한 포인트\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "### **결정 트리 평가**\n",
    "**[장점]**\n",
    "> 1. 만들어진 모델을 쉽게 시각화할 수 있어서 비전문가도 이해하기 쉽다. \n",
    "\n",
    "> 2. 데이터의 스케일에 구애받지 않는다. <br>\n",
    "    * 각 특성이 개별적으로 처리되어 데이터를 분할하기 때문에, 데이터 스케일의 영향을 받지 않으므로<br>\n",
    "결정 트리에서는 데이터가 특정 범위 안에 들어오도록 하는 정규화(Normalization)나 표준화(Standardization)같은 데이터 전처리 과정이 필요가 없다. \n",
    "    * 특히 특성의 스케일이 서로 다르거나 이진 특성과 연속적인 특성이 혼합되어 있을 때에도 잘 작동한다. \n",
    "    \n",
    "> 3. 학습이 끝난 결정 트리의 작업 속도가 매우 빠르다. \n",
    "\n",
    "> 4. 결정 트리는 모델이 어떻게 훈련되었는지 경로의 해석이 가능하다는 것이다.\n",
    "\n",
    "**[단점]**<br>\n",
    "\n",
    "결정 트리의 단점은 사전 가지치기를 사용함에도 과대적합(Overfitting)되는 경향이 있어 <br>\n",
    "모델의 일반화 성능이 좋지 않다는 것이다. 단일 결정 트리의 이러한 단점을 보완한 것이 결정 트리의 앙상블 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------\n",
    "## **2. 결정트리 앙상블**\n",
    "[참고] https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning-%EB%B0%B0%EA%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting <br>\n",
    "\n",
    "앙상블 학습은 여러 개의 결정 트리(Decision Tree)를 결합하여 하나의 결정 트리보다 더 좋은 성능을 내는 머신러닝 기법입니다. <br>\n",
    "\n",
    "앙상블 학습의 핵심은 여러 개의 약 분류기 (Weak Classifier)를 결합하여 강 분류기(Strong Classifier)를 만드는 것입니다. 그리하여 모델의 정확성이 향상됩니다.\n",
    "\n",
    "- 앙상블 학습법에는 두 가지가 있습니다. 배깅(Bagging)과 부스팅(Boosting)입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1.Bagging\n",
    "Bagging은 Bootstrap Aggregation의 약자입니다.<br>\n",
    "배깅은 샘플을 여러 번 뽑아(Bootstrap) 각 모델을 학습시켜 결과물을 집계(Aggregration)하는 방법입니다. \n",
    "\n",
    "<img src=\"./pic/Bagging_2.PNG\" >\n",
    "\n",
    "- 우선, 데이터로부터 부트스트랩을 합니다. (복원 랜덤 샘플링) \n",
    "- 부트스트랩한 데이터로 모델을 학습시킵니다. \n",
    "- 그리고 학습된 모델의 결과를 집계하여 최종 결과 값을 구합니다.\n",
    "\n",
    "> Categorical Data는 투표 방식(Votinig)으로 결과를 집계하며,  <br>\n",
    "> Continuous Data는 평균으로 집계합니다.\n",
    "\n",
    "==> 배깅 기법을 활용한 모델이 바로 **랜덤 포레스트**입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "### 2-1-1. 랜덤포레스트\n",
    "[참고] <br>\n",
    "https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-5-%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8Random-Forest%EC%99%80-%EC%95%99%EC%83%81%EB%B8%94Ensemble <br>\n",
    "\n",
    "결정 트리의 단점은 훈련 데이터에 오버피팅이 되는 경향이 있다는 것입니다.<br>\n",
    "여러 개의 결정 트리를 통해 랜덤 포레스트를 만들면 오버피팅 되는 단점을 해결할 수 있습니다.\n",
    "\n",
    "### 랜덤포레스트 원리\n",
    "* 모든 Feature를 다 사용해서 결정 트리를 만들면 오버피팅을 야기함.\n",
    "    * 예를 들어 Feature가 30개라고 할때, <br>\n",
    "30개의 Feature를 기반으로 하나의 결정 트리를 만든다면 트리의 가지가 많아질 것이고 오버피팅의 결과를 야기할 것입니다. <br>\n",
    "<br>\n",
    "* 일부 Feature만 선택해서 여러개의 결정트리를 만든다.<br>\n",
    "    * 30개의 Feature 중 랜덤으로 5개의 Feature만 선택해서 하나의 결정 트리를 만들고,<br>\n",
    "또 30개 중 랜덤으로 5개의 Feature를 선택해서 또 다른 결정 트리를 만들고 <br>\n",
    "이렇게 계속 반복하여 여러 개의 결정 트리를 만들 수 있습니다. <br>\n",
    "결정 트리 하나마다 예측 값을 내놓겠죠. 여러 결정 트리들이 내린 예측 값들 중 가장 많이 나온 값을 최종 예측값으로 정합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤포레스트 파라미터\n",
    "* **n_estimators**: 랜덤 포레스트 안의 결정 트리 갯수\n",
    "    * n_estimators는 클수록 좋습니다. \n",
    "    * 결정 트리가 많을수록 더 깔끔한 Decision Boundary가 나오겠죠. 하지만 그만큼 메모리와 훈련 시간이 증가합니다.<br>\n",
    "<br>\n",
    "* **max_features**: 무작위로 선택할 Feature의 개수\n",
    "    * max_features=n_features이면 30개의 feature 중 30개의 feature 모두를 선택해 결정 트리를 만듭니다.\n",
    "    * 단, bootstrap=True이면 30개의 feature에서 복원 추출로 30개를 뽑습니다.<br>\n",
    "    특성 선택의 무작위성이 없어질 뿐 샘플링의 무작위성은 그대로인 것입니다.\n",
    "        * 따라서 **max_features 값이 크면 랜덤 포레스트의 트리들이 매우 비슷**해지고, 가장 두드러진 특성에 맞게 예측을 할 것입니다. \n",
    "        * **max_features 값이 작으면 랜덤 포레스트의 트리들이 서로 매우 달라질 것**입니다. 따라서 오버피팅이 줄어들 것입니다. max_features는 일반적으로 Defalut 값을 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "## 2-2. Boosting\n",
    "부스팅은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법입니다.<br>\n",
    "\n",
    "**[배깅과 부스팅 차이]** <br>\n",
    "배깅은 Deicison Tree1과 Decision Tree2가 서로 독립적으로 결과를 예측합니다.<br>\n",
    "여러 개의 독립적인 결정 트리가 각각 값을 예측한 뒤, 그 결과 값을 집계해 최종 결과 값을 예측하는 방식입니다.<br>\n",
    "\n",
    "* 부스팅은 모델 간 팀워크가 이루어집니다. 처음 모델이 예측을 하면 그 예측 결과에 따라 데이터에 가중치가 부여되고, 부여된 가중치가 다음 모델에 영향을 줍니다. \n",
    "* 잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복합니다\n",
    "\n",
    "<img src=\"./pic/Boosting_2.PNG\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "## 2-3. Boosting, Bagging 비교\n",
    "- 배깅은 병렬로 학습 \n",
    "- 부스팅은 순차적으로 학습. \n",
    "    - 한번 학습이 끝난 후 결과에 따라 가중치를 부여합니다. 그렇게 부여된 가중치가 다음 모델의 결과 예측에 영향을 줍니다.\n",
    "    - 오답에 대해서는 높은 가중치를 부여하고, 정답에 대해서는 낮은 가중치를 부여합니다.\n",
    "        - 따라서 오답을 정답으로 맞추기 위해 오답에 더 집중할 수 있게 되는 것입니다. \n",
    "<br>\n",
    "\n",
    "[부스팅의 장점]<br>\n",
    "- 부스팅은 배깅에 비해 error가 적습니다. 즉, 성능이 좋습니다.  <br>\n",
    "\n",
    "[부스팅의 단점] <br>\n",
    "- 하지만 속도가 느리고 오버 피팅이 될 가능성이 있습니다. \n",
    "\n",
    "> 배깅과 부스팅 중 어떤 것을 선택해야 할까 <br>\n",
    "* 개별 결정 트리의 낮은 성능이 문제라면 부스팅이 적합하고, \n",
    "* 오버 피팅이 문제라면 배깅이 적합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suhyun3",
   "language": "python",
   "name": "suhyun3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
