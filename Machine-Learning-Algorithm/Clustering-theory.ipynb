{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "2020-03-06\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터링 기법 정리\n",
    "[참고] https://brunch.co.kr/@gimmesilver/40 <br>\n",
    "\n",
    "<img src=\"./pic/Clustering기법비교.PNG\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "## k-means clustering\n",
    "\n",
    "### K평균 알고리즘의 주요 특징\n",
    "> 1. 좌표 기반의 군집분석 방법\n",
    "    * 좌표 기반의 거리측정 방식\n",
    "2. 대용량 데이터 청리에 유용\n",
    "    * k개 그룹에 대한 거리 측정이나, 중심점 갱신등의 작업은 k개의 시스템으로 분산하여 계산할 수 있음.\n",
    "3. 각 유형의 특징을 파악하기에 좋음\n",
    "    * 각 유형을 나눌 때 사용한 k개의 중심점이 곧 해당 유형의 대표값이기 때문에 유형을 파악하기에 좋음.\n",
    "    \n",
    "### k평균 알고리즘의 한계점\n",
    "> 1. 아웃라이어에 민감하다.\n",
    "2. 유형별 데이터의 분산이 비슷하고, 구형으로 분포되어 있는 경우가 아니라면 부적절한 결과를 낼 수 있다.\n",
    "    * 전체 유형이 동일한 분산과 분포를 갖는다고 가정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "## DBSCAN\n",
    "### 밀도기반 클러스터링\n",
    "\n",
    "개체들의 밀도를 계산하여 촘촘하게 분포되어 있는 개체들끼리 그룹을 묶는 방식. <br>\n",
    "이 알고리즘에서는 두 개의 파라미터를 정해주는데, 하나는 밀도를 계산할 범위(epsilon)이고 <br>\n",
    "또 하나는 하나의 그룹으로 묶는데 필요한 최소 개체수(minPts)이다.\n",
    "\n",
    "### 장점\n",
    "> k평균 알고리즘처럼 데이터의 분포가 구형이고, 분산이 일정해야 한다는 제약이 없음. <br>\n",
    "몇개의 그룹을 묶을 것인지 미리 지정하지 않고 하나의 군집으로 묶을 최소한의 밀도만 지정해주면 알아서 군집을 묶어줌.\n",
    "\n",
    "> **즉, 다양한 형태의 분포에 대해서도 그룹을 묶을 수 있으며, 다른 개체들과 상대적으로 멀리 떨어져 있는 아웃라이어들을 군집에서 제외할 수 있는 장점이 있다.**\n",
    "\n",
    "### 단점\n",
    "> 반면, 각 유형 사이 공간의 데이터 밀도가 충분히 낮지 않다면, <br>\n",
    "다시 말해 각 유형 간의 경계가 모호하면 군집 분석이 잘 안되는 한계를 갖고 있다. <br>\n",
    "* 하나의 유형으로 묶이거나, 혹은 너무 유형이 세밀하게 쪼개지는 문제가 발생할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "## Gaussian-Mixture Clustering\n",
    "\n",
    "> 가우시안 혼합 모델은 전체 데이터의 확률 분포가 여러 개의 가우시안 분포의 조합으로 이뤄져 있다고 가정하고, 각 분포에 속할 확률이 높은 데이터끼리 군집을 묶는 방법이다. <br>\n",
    "\n",
    "#### 가우시안 확률 모델\n",
    "https://sungjk.github.io/2016/01/13/GMM.html <br>\n",
    "이는 하나의 클래스 혹은 관찰된 전체 데이터 집합이 **평균을 중심으로 하여 하나로 뭉쳐져 있는 분포 형태**를 표현하는데 적합한 확률 모델이다.\n",
    "\n",
    "#### 가우시안 확률모델 단점\n",
    "> 주어진 데이터에 대하여 가우시안 확률분포를 이용하여 모델을 설정하는 것은 가장 널리 사용되는 방법이다. <br>\n",
    "그러나 가우시안 확률분포는 기본적으로 데이터들이 평균을 중심으로 하나의 그룹으로 뭉쳐 있는 유니모달(unimodal) 형태를 가진다는 것을 가정하고 있어서, <br>\n",
    "복잡한 분포 형태를 가지는 데이터의 확률밀도함수를 표현하기는 힘들다는 문제점이 있다.\n",
    "\n",
    "### 가우시안 혼합 모델 (Gaussian Mixture Model)\n",
    "복잡한 데이터 분포를 추정하기 위해서는 보다 일반적인 형태를 표현할 수 있는 확률 모델이 필요하며,<br>\n",
    "이때 가장 손쉽게 생각해 볼 수 있는 것이 여러 개의 가우시안을 합하여 만들어지는 모델이다<br>\n",
    "이를 가우시안 혼합 모델(Gaussian Mixture Model) 이라고 한다.\n",
    "\n",
    "* 복수 개의 가우시안 분포들의 합으로 새로운 확률분포를 나타내는 가우시안 혼합 모델을 사용하면, <br>\n",
    "하나의 가우시안 분포함수로 나타낼 수 없었던 분포 특성을 잘 나타낼 수 있을 뿐만 아니라,<br>\n",
    "아무리 복잡한 형태의 함수라도 충분한 개수의 가우시안 함수를 사용하기만 하면 원하는 만큼 정확하게 근사해 낼 수 있다.\n",
    "\n",
    "<img src=\"./pic/GaussianModel.PNG\" width=\"300\">\n",
    "\n",
    "### 가우시안 혼합모델 단점\n",
    "> 데이터가 정규 분포의 조합으로 표현된다는 가정 하에 분석하는 방식이기 때문에, 이 가정에 어긋나는 데이터라면 클러스터링이 부적절하게 될 수 있다. <br>\n",
    "또한 이 기법은 계산량이 많기 때문에 대량의 데이터에 사용하기에 적절하지 않다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "## k-medoid clustering\n",
    "PAM(Partitioning around medoids)\n",
    "* k-medoid 알고리즘을 가장 일반적으로 실현화한 것중 대표적인것은 PAM(Partitoning Around Medoid) 알고리즘\n",
    "\n",
    "> k-means clustering에서 각 군집을 centroid(변수들의 평균 벡터)로 나타내는 것과 달리 <br>\n",
    "각 군집은 하나의 관찰치(medoid라고 부른다)로 대표된다. <br>\n",
    "k-mean에서 유클리드 거리를 사용하는 것과 달리 PAM에서는 다른 거리 측정법도 사용할 수 있기 때문에<br>\n",
    "연속형 변수들 뿐만 아니라 mixed data type에도 적합시킬 수 있다.\n",
    "\n",
    "### PAM 알고리즘\n",
    "1. K개의 관찰치(medoid)를 무작위로 선택한다.\n",
    "2. 모든 관찰치에서 각medoid까지의 거리를 계산한다.\n",
    "3. 각 관찰치를 가장 가까운 medoid에 할당한다.\n",
    "4. 각 관찰치와 해당하는 medoid사이의 거리의 총합(총비용,total cost)을 계산한다.\n",
    "5. medoid가 아닌 점 하나를 선택하여 그 점에 할당된 medoid와 바꾼다.\n",
    "6. 모든 관찰치들을 가장 가까운 medoid에 할당한다.\n",
    "7. 총비용을 다시 계산한다.\n",
    "8. 다시계산한 총비용이 더 작다면 새 점들을 medoid로 유지한다.\n",
    "9. medoid가 바뀌지 않을 때까지 5-8단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "## k-median clustering\n",
    "k-중앙값 클러스터링은 각 클러스터의 무게중심을 구하기 위해 평균을 사용하는 대신 중앙값을 사용한다.<br>\n",
    "L2 거리를 최소화하는 k-평균과는 달리, k-중앙값 클러스터링은 L1 거리를 최소화 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suhyun3",
   "language": "python",
   "name": "suhyun3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
